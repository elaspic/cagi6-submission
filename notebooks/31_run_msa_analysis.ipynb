{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"31_run_msa_analysis\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "\n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = max(1, len(os.sched_getaffinity(0)))\n",
    "\n",
    "CPU_COUNT = max(1, CPU_COUNT // 2)\n",
    "\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = os.getenv(\"DATASET_NAME\")\n",
    "DATASET_PATH = os.getenv(\"DATASET_PATH\")\n",
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "DATASET_NAME, DATASET_PATH, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    DATASET_NAME = \"cagi6-sherloc\"\n",
    "    DATASET_PATH = str(\n",
    "        # NOTEBOOK_DIR.parent.joinpath(\"30_cagi6_sherloc\", \"input-data-gby-protein.parquet\")\n",
    "        NOTEBOOK_DIR.parent.joinpath(\"30_humsavar\", \"humsavar-gby-protein-waln.parquet\")\n",
    "    )\n",
    "    TASK_ID = 1098\n",
    "    TASK_COUNT = 12557 # 4182\n",
    "else:\n",
    "    assert DATASET_NAME is not None\n",
    "    assert DATASET_PATH is not None\n",
    "    DATASET_PATH = Path(DATASET_PATH).expanduser().resolve()\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "DATASET_NAME, DATASET_PATH, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = NOTEBOOK_DIR.joinpath(DATASET_NAME, f\"result-{TASK_ID}-of-{TASK_COUNT}.parquet\")\n",
    "output_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_file.is_file():\n",
    "    raise Exception(\"Already finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = pq.ParquetFile(DATASET_PATH)\n",
    "\n",
    "pfile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(1.4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_per_chunk = np.ceil(pfile.num_row_groups / TASK_COUNT).astype(int)\n",
    "\n",
    "rows_per_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (TASK_ID - 1) * rows_per_chunk\n",
    "stop = min([pfile.num_row_groups + 1, TASK_ID * rows_per_chunk])\n",
    "\n",
    "start, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = \"ARNDCEQGHILKMFPSTWYV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_counts(sequences):\n",
    "    counts_mat = np.zeros((len(sequences[0]), len(AMINO_ACIDS)), dtype=np.float64)\n",
    "    for i, msa_row in enumerate(zip(*sequences)):\n",
    "        msa_row = [aa for aa in msa_row if aa in AMINO_ACIDS]\n",
    "        counts = Counter(msa_row)\n",
    "        counts_mat[i, :] = [counts.get(aa, 0) for aa in AMINO_ACIDS]\n",
    "    return counts_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_to_probas(counts_mat):\n",
    "    probas_mat = np.log((counts_mat + 1) / (counts_mat.sum(axis=1, keepdims=True) + 20))\n",
    "    return probas_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_convervation_script(sequences):\n",
    "    script_name = \"Conservation.jl\"\n",
    "    fasta_string = \"\".join((f\">{i}\\n{seq}\\n\" for i, seq in enumerate(sequences)))\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_path = Path(tmp_dir)\n",
    "        shutil.copy(NOTEBOOK_DIR.joinpath(script_name), tmp_path.joinpath(script_name))\n",
    "        with tmp_path.joinpath(\"aln.fasta\").open(\"wt\") as fout:\n",
    "            fout.write(fasta_string)\n",
    "        cmd = [\"julia\", script_name, \"-f\", \"FASTA\", \"aln.fasta\"]\n",
    "        proc = subprocess.run(\n",
    "            cmd, cwd=tmp_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True\n",
    "        )\n",
    "        result_df = pd.read_csv(tmp_path.joinpath(\"aln.fasta.conservation.csv\"), comment=\"#\")\n",
    "\n",
    "    assert len(result_df) == len(sequences[0])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Row:\n",
    "    protein_id: str = None\n",
    "    counts_mat: np.ndarray = None\n",
    "    probas_mat: np.ndarray = None\n",
    "    msa_length: int = None\n",
    "    msa_proba: float = None\n",
    "    conservation_df: pd.DataFrame = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = None\n",
    "previous_row = None\n",
    "for task_idx in tqdm(range(start, stop)):\n",
    "    input_df = pfile.read_row_group(task_idx).to_pandas(integer_object_nulls=True)\n",
    "    assert len(input_df) == 1\n",
    "    tup = next(input_df.itertuples())\n",
    "    sequences = [\n",
    "        \"\".join((aa for aa in line.strip() if not aa.islower()))\n",
    "        for line in tup.alignment\n",
    "        if line and not line.startswith(\">\")\n",
    "    ]\n",
    "    assert tup.sequence == sequences[0]\n",
    "    assert all([len(tup.sequence) == len(seq) for seq in sequences])\n",
    "\n",
    "    if previous_row is not None and previous_row.protein_id == tup.protein_id:\n",
    "        row = previous_row\n",
    "    else:\n",
    "        row = Row()\n",
    "        row.protein_id = tup.protein_id\n",
    "        row.counts_mat = sequences_to_counts(sequences)\n",
    "        row.probas_mat = counts_to_probas(row.counts_mat)\n",
    "        row.msa_length = len(sequences)\n",
    "        row.msa_proba = np.mean(\n",
    "            [row.probas_mat[i, AMINO_ACIDS.index(aa)] for i, aa in enumerate(sequences[0])]\n",
    "        )\n",
    "        row.conservation_df = run_convervation_script(sequences)\n",
    "\n",
    "    mutation_results = []\n",
    "    for mutation_id, mutation in zip(tup.mutation_id, tup.mutation):\n",
    "        aa_wt, pos, aa_mut = mutation[0], int(mutation[1:-1]), mutation[-1]\n",
    "        if len(sequences[0]) < pos or sequences[0][pos - 1] != aa_wt:\n",
    "            print(f\"Mutation {mutation!r} does not match sequence for {task_idx=}.\")\n",
    "            continue\n",
    "\n",
    "        cons = row.conservation_df.iloc[pos - 1]\n",
    "        assert cons.i == pos\n",
    "\n",
    "        idx_wt = AMINO_ACIDS.index(aa_wt)\n",
    "        idx_mut = AMINO_ACIDS.index(aa_mut)\n",
    "        mutation_results.append(\n",
    "            {\n",
    "                \"protein_id\": tup.protein_id,\n",
    "                \"mutation_id\": mutation_id,\n",
    "                \"mutation\": mutation,\n",
    "                \"msa_count_wt\": row.counts_mat[pos - 1, idx_wt],\n",
    "                \"msa_count_mut\": row.counts_mat[pos - 1, idx_mut],\n",
    "                \"msa_count_total\": row.counts_mat[pos - 1].sum().item(),\n",
    "                \"msa_proba_wt\": row.probas_mat[pos - 1, idx_wt],\n",
    "                \"msa_proba_mut\": row.probas_mat[pos - 1, idx_mut],\n",
    "                \"msa_proba_total\": row.probas_mat[pos - 1].sum().item(),\n",
    "                \"msa_length\": row.msa_length,\n",
    "                \"msa_proba\": row.msa_proba,\n",
    "                \"msa_H\": cons.H,\n",
    "                \"msa_KL\": cons.KL,\n",
    "            }\n",
    "        )\n",
    "    mutation_results_df = pd.DataFrame(mutation_results)\n",
    "    table = pa.Table.from_pandas(mutation_results_df, preserve_index=False)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_file, table.schema)\n",
    "    writer.write_table(table)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
