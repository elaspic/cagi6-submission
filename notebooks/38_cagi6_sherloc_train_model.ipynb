{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c81e96-6b41-41fa-80a6-6360c482eb58",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec276dec-6afa-4b57-bbfc-a11e04a8767e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f4edf-ca04-4bef-b5c9-a6d402f7570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import tempfile\n",
    "import uuid\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import elaspic2 as el2\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as olgb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from scipy import stats\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b0792-00b7-44be-9257-a554f95bd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 1000)\n",
    "pd.set_option(\"max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0ca05-c038-414f-ba84-b2ae8013088b",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907ef01-3df6-40bc-a669-b74c41b81200",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    UNIQUE_ID\n",
    "except NameError:\n",
    "    UNIQUE_ID = str(uuid.uuid4())[:8]\n",
    "\n",
    "UNIQUE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1faee-9f6a-40d8-8819-272b1c9837bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"38_cagi6_sherloc_train_model\").resolve()\n",
    "NOTEBOOK_DIR.joinpath(UNIQUE_ID).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bb004-a4ad-4780-bcbb-360c6c8ca1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALPHAFOLD = \"wt\"\n",
    "assert USE_ALPHAFOLD in [None, \"wt\", \"wt+mut\"]\n",
    "\n",
    "USE_ALPHAFOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00941b-a3df-48d4-864c-bef984a8cd3b",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eef855-210a-46c5-b48a-6e25b8986b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = NOTEBOOK_DIR.parent.joinpath(\n",
    "    \"37_cagi6_sherloc_combine_results\", \"combined-results.parquet\"\n",
    ")\n",
    "\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33ba1c-76ed-4d2f-9e9f-0f36376ff50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pq.read_table(input_file).to_pandas()\n",
    "\n",
    "display(result_df.head(2))\n",
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87c286-d491-42d4-941d-606cde544d48",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1374e04-eae5-47bb-b1b0-9aafc45782a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinsolver_columnms = [c for c in result_df if c.startswith(\"proteinsolver_\")]\n",
    "protbert_columns = [c for c in result_df if c.startswith(\"protbert_\")]\n",
    "rosetta_columns = [c for c in result_df if c.startswith(\"rosetta_\")]\n",
    "alphafold_columns = [c for c in result_df if c.startswith(\"alphafold_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448fdf8-4949-4e91-b6c2-0db0711b8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"effect\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8e99e-ccd9-4c28-8613-c3bde79aefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_map = {\n",
    "    \"Uncertain significance\": 0,\n",
    "    \"Likely benign\": -1,\n",
    "    \"Benign\": -2,\n",
    "    \"Likely pathogenic\": 1,\n",
    "    \"Pathogenic\": 2,\n",
    "}\n",
    "\n",
    "result_df[\"effect_score\"] = result_df[\"effect\"].map(effect_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb146d6b-4dac-451b-8ed2-951a07d51a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [\n",
    "    \"el2_score\",\n",
    "    \"proteinsolver_core_score_change\",\n",
    "    \"protbert_core_score_change\",\n",
    "    \"rosetta_dg_change\",\n",
    "    \"alphafold_core_scores_residue_plddt_wt\",\n",
    "    \"alphafold_core_scores_proten_ptm_wt\",\n",
    "    #         \"alphafold_core_scores_residue_plddt_change\",\n",
    "    #         \"alphafold_core_scores_protein_max_predicted_aligned_error_wt\",\n",
    "]\n",
    "\n",
    "df = result_df.dropna(subset=score_columns + [\"effect_score\"])\n",
    "df = df[df[\"effect_score\"] != 0].reset_index(drop=True)\n",
    "\n",
    "for col in score_columns:\n",
    "    corr = stats.spearmanr(df[\"effect_score\"], df[col])\n",
    "    auc = metrics.roc_auc_score(df[\"effect_score\"] > 0, df[col])\n",
    "    precision = metrics.average_precision_score(df[\"effect_score\"] > 0, df[col])\n",
    "    print(col, corr[0], auc, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6e550-dd13-43fc-a6f9-161faa113c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [\n",
    "    \"el2_score\",\n",
    "    \"proteinsolver_core_score_change\",\n",
    "    \"protbert_core_score_change\",\n",
    "    \"msa_KL\",\n",
    "    \"rosetta_dg_change\",\n",
    "    \"alphafold_core_scores_residue_plddt_wt\",\n",
    "    \"alphafold_core_scores_residue_plddt_change\",\n",
    "    #         \"alphafold_core_scores_protein_plddt_wt\",\n",
    "    #         \"alphafold_core_scores_protein_max_predicted_aligned_error_wt\",\n",
    "    #         \"alphafold_core_scores_proten_ptm_wt\",\n",
    "]\n",
    "\n",
    "for column in score_columns:\n",
    "    print(f\"{column} {result_df[column].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a022f7-5f44-4505-9d5b-7f2086b984aa",
   "metadata": {},
   "source": [
    "## Train ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477fa31-30ac-4db5-b7f3-f063ed962d47",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4232703-0484-493b-be75-ca71af0aa795",
   "metadata": {},
   "outputs": [],
   "source": [
    "rosetta_columns = [c for c in result_df if \"rosetta\" in c]\n",
    "msa_columns = [c for c in result_df if c.startswith(\"msa\")]\n",
    "\n",
    "len(rosetta_columns), len(msa_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79212bed-537c-40f7-8598-8e92fcface6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphafold_columns = []\n",
    "\n",
    "if USE_ALPHAFOLD and \"wt\" in USE_ALPHAFOLD:\n",
    "    alphafold_columns += [\n",
    "        \"alphafold_core_scores_residue_plddt_wt\",\n",
    "        \"alphafold_core_scores_proten_ptm_wt\",\n",
    "    ]\n",
    "\n",
    "if USE_ALPHAFOLD and \"mut\" in USE_ALPHAFOLD:\n",
    "    alphafold_columns += [\n",
    "        \"alphafold_core_scores_residue_plddt_change\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32d346-15dc-44b4-81d0-66c9b939dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_features = (\n",
    "    [\n",
    "        # \"el2_score\",\n",
    "        \"protbert_core_score_wt\",\n",
    "        \"protbert_core_score_change\",\n",
    "        \"proteinsolver_core_score_wt\",\n",
    "        \"proteinsolver_core_score_change\",\n",
    "    ]\n",
    "    + msa_columns\n",
    "    + rosetta_columns\n",
    "    + alphafold_columns\n",
    ")\n",
    "\n",
    "with NOTEBOOK_DIR.joinpath(UNIQUE_ID, \"scalar-features.json\").open(\"wt\") as fout:\n",
    "    json.dump(scalar_features, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fca4d3-4f90-4f63-b93f-91bc81b101c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_features = [\n",
    "    \"aa_wt_onehot\",\n",
    "    \"aa_mut_onehot\",\n",
    "    \"protbert_core_features_residue_wt\",\n",
    "    \"protbert_core_features_residue_change\",\n",
    "    \"proteinsolver_core_features_residue_wt\",\n",
    "    \"proteinsolver_core_features_residue_change\",\n",
    "]\n",
    "\n",
    "\n",
    "if USE_ALPHAFOLD and \"wt\" in USE_ALPHAFOLD:\n",
    "    vector_features += [\n",
    "        \"alphafold_core_features_residue_experimentally_resolved_wt\",  # 0.19 [37]\n",
    "        \"alphafold_core_features_residue_predicted_lddt_wt\",  # 0.17 [50]\n",
    "        \"alphafold_core_features_residue_msa_first_row_wt\",  # 0.17 [256]\n",
    "        \"alphafold_core_features_residue_single_wt\",  # 0.20 [384]\n",
    "        \"alphafold_core_features_residue_structure_module_wt\",  # 0.18 [384]\n",
    "    ]\n",
    "\n",
    "if USE_ALPHAFOLD and \"mut\" in USE_ALPHAFOLD:\n",
    "    vector_features += [\n",
    "        \"alphafold_core_features_residue_experimentally_resolved_change\",  # 0.11 [37]\n",
    "        \"alphafold_core_features_residue_predicted_lddt_change\",  # 0.04 [50]\n",
    "        \"alphafold_core_features_residue_msa_first_row_change\",  # 0.21 [256]\n",
    "        \"alphafold_core_features_residue_single_change\",  # 0.15 [384]\n",
    "        \"alphafold_core_features_residue_structure_module_change\",  # 0.05 [384]\n",
    "    ]\n",
    "\n",
    "with NOTEBOOK_DIR.joinpath(UNIQUE_ID, \"vector-features.json\").open(\"wt\") as fout:\n",
    "    json.dump(vector_features, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489a420-8666-4c13-a45c-4a0c0e05ae2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    result_df.dropna(\n",
    "        subset=scalar_features\n",
    "        + vector_features\n",
    "        + [\n",
    "            \"effect_score\",\n",
    "        ]\n",
    "    )\n",
    "    .drop_duplicates(subset=[\"protein_id\", \"mutation\"])\n",
    "    .sort_values(\"protein_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df = df[df[\"effect_score\"] != 0]\n",
    "\n",
    "# protein_map = {k: i for i, k in enumerate(df[\"protein_id\"].unique())}\n",
    "# groups = df[\"protein_id\"].map(protein_map).values\n",
    "\n",
    "value_counts = df[\"protein_id\"].value_counts()\n",
    "groups = df[\"protein_id\"].drop_duplicates().map(value_counts)\n",
    "\n",
    "X_ref = np.c_[\n",
    "    df[scalar_features].values,\n",
    "    np.hstack([np.vstack(df[col].values) for col in vector_features]),\n",
    "]\n",
    "X = X_ref\n",
    "# X = X[:, important_features]\n",
    "\n",
    "low_confidence_mask = df[\"effect_score\"] == 0\n",
    "\n",
    "y = (df[\"effect_score\"] > 0).values.astype(int)\n",
    "y[low_confidence_mask] = (df[low_confidence_mask][\"el2_score\"] > 2).values.astype(int)\n",
    "# y = df[\"effect_score\"].values\n",
    "\n",
    "# weights = np.ones(len(df), dtype=np.float64)\n",
    "# weights[df[\"effect_score\"] == 1] = 0.5\n",
    "# weights[df[\"effect_score\"] == -1] = 0.5\n",
    "# weights[low_confidence_mask] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2458b-420a-434d-b7fe-41842e9f4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268b00d-6f53-489a-a8aa-8141622fcec1",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe1ca1-f5ef-42c1-82d9-0edad9fdd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_PARAM = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"is_unbalance\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f2323-0afa-4eb4-937d-2e77e6a2b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X, y, param, n_splits=6, progressbar=False):\n",
    "    models = []\n",
    "    preds = np.ones(len(y), dtype=np.float64) * np.nan\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    for train_index, test_index in tqdm(\n",
    "        gkf.split(X, y, groups=df[\"protein_id\"]),\n",
    "        total=n_splits,\n",
    "        disable=not progressbar,\n",
    "    ):\n",
    "        X_training, X_testing = X[train_index], X[test_index]\n",
    "        y_training, y_testing = y[train_index], y[test_index]\n",
    "        # weights_training, weights_testing = weights[train_index], weights[test_index]\n",
    "\n",
    "        dtrain = lgb.Dataset(\n",
    "            X_training,\n",
    "            label=y_training,\n",
    "            # weight=weights_training,\n",
    "        )\n",
    "        model = lgb.train(param, dtrain)\n",
    "        preds[test_index] = model.predict(X_testing)\n",
    "        models.append(model)\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde5332-4896-4fd2-bdee-cf1b8f9f4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, low_confidence_mask):\n",
    "    param = CONST_PARAM | {\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 64),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 60),\n",
    "    }\n",
    "    models, preds = training_loop(X, y, param)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = metrics.accuracy_score(y[~low_confidence_mask], pred_labels[~low_confidence_mask])\n",
    "    auc = metrics.roc_auc_score(y[~low_confidence_mask], preds[~low_confidence_mask])\n",
    "    precision = metrics.average_precision_score(\n",
    "        y[~low_confidence_mask], preds[~low_confidence_mask]\n",
    "    )\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af684e6-ff75-4540-ac7c-81487cebd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X, label=y, group=groups)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as model_dir:\n",
    "    tuner = olgb.LightGBMTunerCV(\n",
    "        CONST_PARAM | {\"verbosity\": -1},\n",
    "        dtrain,\n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=250,\n",
    "        folds=GroupKFold(n_splits=6),\n",
    "        num_boost_round=1000,\n",
    "        model_dir=model_dir,\n",
    "        time_budget=60 * 60 * 2,\n",
    "        return_cvbooster=True,\n",
    "    )\n",
    "    tuner.run()\n",
    "    booster = tuner.get_best_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb86b3b-89ff-4659-9d18-52650dc9a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = tuner.best_params\n",
    "\n",
    "print(tuner.best_score)\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa394afb-ad24-4544-99e7-b082947813bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NOTEBOOK_DIR.joinpath(UNIQUE_ID, \"best-parameters-starting.json\").open(\"wt\") as fout:\n",
    "    json.dump(param, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c57b7-29eb-4984-bb8c-36129a227603",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bst_idx, bst in enumerate(booster.boosters):\n",
    "    bst.save_model(str(NOTEBOOK_DIR.joinpath(UNIQUE_ID, f\"model-starting-{bst_idx}.txt\")))\n",
    "\n",
    "_ = dtrain.save_binary(str(NOTEBOOK_DIR.joinpath(UNIQUE_ID, \"training-data.bin\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523fa04-0c78-46cb-b7ef-b07c34bfdfe5",
   "metadata": {},
   "source": [
    "### Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c43031-7571-4491-b959-562b11e7d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_index(num_features, features_to_exclude):\n",
    "    idx = 0\n",
    "    seen = set(features_to_exclude)\n",
    "    indices = list(reversed(range(num_features)))\n",
    "    while indices:\n",
    "        idx = indices.pop()\n",
    "        while idx in seen:\n",
    "            idx += 1\n",
    "        seen.add(idx)\n",
    "        yield idx\n",
    "\n",
    "\n",
    "list(get_feature_index(10, {1, 2, 3, 9, 10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ae48d-220f-41a0-995d-7c82e63ddb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_elimination_stats_file = NOTEBOOK_DIR.joinpath(\n",
    "    UNIQUE_ID, \"feature-elimination-stats.pickle\"\n",
    ")\n",
    "feature_elimination_stats_file.touch()\n",
    "\n",
    "features_to_exclude = {}\n",
    "\n",
    "fe_round = -1\n",
    "fe_round_stats = []\n",
    "best_params = {3000: None, 2000: None, 1000: None, 500: None}\n",
    "highest_precision = None\n",
    "while len(features_to_exclude) < X_ref.shape[1]:\n",
    "    fe_round += 1\n",
    "\n",
    "    # Apply feature elimination mask\n",
    "    feature_mask = np.ones(X_ref.shape[1], dtype=bool)\n",
    "    feature_mask[np.array(list(features_to_exclude), dtype=int)] = False\n",
    "    X = X_ref[:, feature_mask]\n",
    "    assert len(features_to_exclude) == X_ref.shape[1] - X.shape[1]\n",
    "\n",
    "    # Retune parameters\n",
    "    for cutoff, best_param in list(best_params.items()):\n",
    "        if X.shape[1] < cutoff and best_param is None:\n",
    "            dtrain = lgb.Dataset(X, label=y, group=groups)\n",
    "            tuner = olgb.LightGBMTunerCV(\n",
    "                CONST_PARAM | {\"verbosity\": -1},\n",
    "                dtrain,\n",
    "                verbose_eval=200,\n",
    "                early_stopping_rounds=250,\n",
    "                folds=GroupKFold(n_splits=6),\n",
    "                num_boost_round=1000,\n",
    "                time_budget=60 * 60 * 2,\n",
    "            )\n",
    "            tuner.run()\n",
    "            best_params[cutoff] = tuner.best_params\n",
    "            param = tuner.best_params\n",
    "            with NOTEBOOK_DIR.joinpath(UNIQUE_ID, f\"best-parameters-{cutoff}.json\").open(\n",
    "                \"wt\"\n",
    "            ) as fout:\n",
    "                json.dump(param, fout)\n",
    "\n",
    "    # Re-train models\n",
    "    models, preds = training_loop(X, y, param, progressbar=True)\n",
    "\n",
    "    # Calculate model statistics\n",
    "    corr = stats.spearmanr(y[~low_confidence_mask], preds[~low_confidence_mask])\n",
    "    auc = metrics.roc_auc_score(y[~low_confidence_mask], preds[~low_confidence_mask])\n",
    "    precision = metrics.average_precision_score(\n",
    "        y[~low_confidence_mask], preds[~low_confidence_mask]\n",
    "    )\n",
    "\n",
    "    # Save stats\n",
    "    round_stats = (\n",
    "        fe_round,\n",
    "        corr[0],\n",
    "        auc,\n",
    "        precision,\n",
    "        len(features_to_exclude),\n",
    "        X.shape[1],\n",
    "        list(features_to_exclude),\n",
    "    )\n",
    "    print(round_stats[:-1])\n",
    "    fe_round_stats.append(round_stats)\n",
    "    shutil.copyfile(\n",
    "        feature_elimination_stats_file, feature_elimination_stats_file.with_suffix(\".pickle.bak\")\n",
    "    )\n",
    "    with feature_elimination_stats_file.open(\"wb\") as fout:\n",
    "        pickle.dump(fe_round_stats, fout, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Save best models\n",
    "    if highest_precision is None or precision > highest_precision:\n",
    "        highest_precision = precision\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.save_model(\n",
    "                str(NOTEBOOK_DIR.joinpath(UNIQUE_ID, f\"model-{fe_round}-{model_idx}.txt\"))\n",
    "            )\n",
    "\n",
    "    # Find new features to eliminate\n",
    "    feature_importance_split = np.vstack(\n",
    "        [model.feature_importance(\"split\") for model in models]\n",
    "    ).sum(axis=0)\n",
    "    feature_importance_gain = np.vstack([model.feature_importance(\"gain\") for model in models]).sum(\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    feature_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature_idx\": list(\n",
    "                get_feature_index(len(feature_importance_split), features_to_exclude)\n",
    "            ),\n",
    "            \"feature_importance_split\": feature_importance_split,\n",
    "            \"feature_importance_gain\": feature_importance_gain,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (feature_df[\"feature_importance_split\"] == 0).any():\n",
    "        feature_df = feature_df[feature_df[\"feature_importance_split\"] == 0]\n",
    "    else:\n",
    "        num_features_to_drop = max(1, len(feature_df) // 100)\n",
    "        feature_df = feature_df.sort_values(\n",
    "            [\"feature_importance_split\", \"feature_importance_gain\"], ascending=True\n",
    "        ).iloc[:num_features_to_drop]\n",
    "    new_features_to_exclude = feature_df[\"feature_idx\"].values.tolist()\n",
    "\n",
    "    for feature_idx in new_features_to_exclude:\n",
    "        assert feature_idx not in features_to_exclude\n",
    "        features_to_exclude[feature_idx] = fe_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5609f-c947-419a-9917-5a021ffb0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f2a22-cc25-4328-8723-729c32168584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
